<!DOCTYPE html>
<html lang="en">
<head>
  <style>
    div {
      max-width: 1000px;
      min-width: 600px;
    }
  </style>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Document</title>
</head>
<body>
  <center>
  <h1> Differentiable Artificial Reverberation  </h1>

    Sungho Lee, Hyeong-Seok Choi, and Kyogu Lee <br>
    Music and Audio Research Group, Seoul National University, Seoul, Republic of Korea <br>
    <br>
    <b>ABSTRACT</b>
  </center>
  
  We propose differentiable artificial reverberation (DAR), a family of artificial reverberation (AR) models implemented in a deep learning framework. 
  Combined with the modern deep neural networks (DNNs), the differentiable structure of DAR allows training loss gradients to be back-propagated in an end-to-end manner.
  Most of the AR models are composed of infinite impulse response (IIR) filters, which bottleneck training speed like any other recurrent layer when implemented ``as is'' in the time domain and executed with a modern parallel processor like GPU. 
  We tackle this issue by further developing a recently proposed acceleration technique, which borrows the frequency-sampling method (FSM).
  With the proposed DAR models, we aim to solve an artificial reverberation parameter (ARP) estimation task in a unified approach: we design an ARP estimation network architecture applicable to both analysis-synthesis (RIR-to-ARP) and blind estimation (reverberant-speech-to-ARP) tasks with any AR model.
  This way, the proposed DAR framework overcomes the previous methods' limitations of task-dependency and AR-model-dependency.
  
  <center>
  <br>
  <img src="Final.png" alt="Framework">
  <br>
  <br>
  Proposed Framework.
  <br>
  <br>
  <h3> Differentiable Filtered Velvet Noise </h3>

  <table>
  <tr>
    <th>Task.</th>
    <th>No.</th>
    <th>True RIR</th>
    <th>Dry Speech</th>
    <th>Reverberant Speech</th>
    <th>Differentiable Estimation</th>
    <th>Convolved Speech<br>w/ Differentiable Estimation</th>
    <th>True LTI Estimation</th>
    <th>Convolved Speech<br>w/ True LTI Estimation</th>
  </tr>
  <tr>
    <td>Analysis-synthesis</td>
    <td>1</td>
    <td><audio controls style="width: 200px;"><source src="audio/fvn/as/000/2_rir.wav" type="audio/wav"></audio></td>
    <td><audio controls style="width: 200px;"><source src="audio/fvn/as/000/2_dry_speech.wav" type="audio/wav"></audio></td>
    <td><audio controls style="width: 200px;"><source src="audio/fvn/as/000/2_transmitted_speech.wav" type="audio/wav"></audio></td>
    <td><audio controls style="width: 200px;"><source src="audio/fvn/as/000/2_freq_sampled_estimation.wav" type="audio/wav"></audio></td>
    <td><audio controls style="width: 200px;"><source src="audio/fvn/as/000/2_freq_sampled_convolved_speech.wav" type="audio/wav"></audio></td>
    <td><audio controls style="width: 200px;"><source src="audio/fvn/as/000/2_true_estimation.wav" type="audio/wav"></audio></td>
    <td><audio controls style="width: 200px;"><source src="audio/fvn/as/000/2_true_convolved_speech.wav" type="audio/wav"></audio></td>
  </tr>
  </table>
  
  <h3> Advanced Filtered Velvet Noise </h3>
  </center>
</body>
</html>
